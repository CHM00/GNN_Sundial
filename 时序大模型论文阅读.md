# 时序大模型论文阅读

## Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting

![image-20251124175810665](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251124175810665.png)

Informer是对Transformer的改造，分为Encoder和Decoder两部分。Informer相对Transformer的主要改进如下：

1. 针对Self-attention计算复杂度高的问题：提出Prob Sparse Self-attention，筛选出Active query，降低计算复杂度；
2. 针对Attention Stack内存过高的问题：提出Self-attention Distilling，减少维度和网络参数量；
3. 原始Transformer的Decoder是**step-by-step逐步解码**，运行很慢：提出**生成式Decoder**，一次性生成长序列预测。

### ProbSparse 自注意力机制 

**ProbSparse 自注意力机制 (ProbSparse self-attention mechanism)** 用于解决传统自注意力机制的二次方计算和内存复杂度问题，该机制的核心在于识别并只关注那些"主导性"的点积对，从而实现稀疏化。

**Informer**使用 **Kullback-Leibler (KL) 散度** 来衡量均匀分布和注意力概率分布之间的距离，目的是识别那些”**激活**“或”**重要**“的查询（Query）, 即**查询稀疏性度量**。

**==注意力分布的稀疏性==**：通过定性评估发现，自注意力机制中学习到的注意力分数形成了长尾分布（long tail distribution）, 这意味着少数**点积对**贡献了主要的注意力，而其他点积对产生的注意力是微不足道的。

**==区分注意力的稀疏性==**：由于注意力机制得出的结论是：第 *i* 个查询 $q_i$ 对所有键 *K* 的注意力被定义为一个概率分布 $p(k_j∣q_i)$； **”非重要查询特征“**指的是，如果 $p(k_j∣q_i)$接近均匀分布$q(k_j∣q_i)=1/L_K$，则自注意力机制的结果将变成值得求和。**”重要特征“**指的是，主导性得点积对会促使相应的注意力概率分布远离均匀分布。

**==KL散度==**：利用**KL散度**，即使用**概率分布**$p$和**均匀分布**$q$之间的相似性来区分**重要查询**。







### Decoder: 一次性生成长序列输出

**Step1: 构造输入**

设**Encoder**输入为：$X_{enc}$=32 * 96 * 7, 那么**Decoder**的输入为：$X_{dec}=32*72*7$（32个batch，72个时间点的数据，每个时间点的数据对应7维的特征）。$X_{dec}$构造方式如下：

（1）选取被预测序列之前的一小段数据作为start_token：从$X_{enc}$的后半段开始取，比如从$X_{enc}$ 的时间点49 ~ 时间点96 取共48个时间点的数据。

（2）**需要预测的数据用0 mask掉**：从时间点97 ~ 时间点120 取共24个时间点的数据，**当然这部分的数据的目标变量是未知且待预测的**

![img](https://i-blog.csdnimg.cn/blog_migrate/8282c0f7d12b4571ea5a748537e14ac3.png#pic_center)



**Step2: 带掩码的概率稀疏自注意力机制**

（1）用Decoder的输入做mask attention，在decoder输入中，当前时间点的$q$只和当前时间点之前的$k$做attention，相当于$K^TQ$矩阵中只有$k^T_iq_j(i<=j)!=0$，矩阵其余部分为0。

（2）*Lazy query* 对应的输出 $o_j$ 不再用所有时间点的 $v$ 的均值填充，而是用**当前时间点**之间的所有 $v$ 的**累加值**填充。

（3）交叉注意力机制 **Cross Self Attention**, K、V是Encoder的输出，Q来自上一层的输出



## Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting

![image-20251123213446438](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251123213446438.png)



### Autoformer的核心：基于周期的依赖发现（Period-based dependencies）

时间序列通常有周期性（比如交通流的日周期、周周期）。处于“相同相位”的子序列（比如“这周五晚高峰”和“上周五晚高峰”）在形状上是高度相似的。我们的目标不是去关注所有历史数据，而是把**注意力集中在**那些**与当前时刻处于同一相位**的历史片段上。

自相关公式($R_XX(\tau)$):
$$
\mathcal{R}_{\mathcal{XX}}(\tau) = \lim_{L \to \infty} \frac{1}{L} \sum_{t=1}^{L} \mathcal{X}_t \mathcal{X}_{t-\tau}
$$
**$\tau$ (Time Lag/Delay):** 这是一个滑动窗口的步长, 测试序列与其自身延迟了 $\tau$ 步后的版本之间的关系。$\sum_{t=1}^{L} \mathcal{X}_t \mathcal{X}_{t-\tau}$是衡量两个序列的相似度，如果序列在滞后 $\tau$ 时发生了重合（比如 $\tau=24$小时），波峰对波峰，波谷对波谷，乘积均为正，总和（$\mathcal{R}$）就会非常大。如果 $\tau$ 不是周期（比如 $\tau=13$小时），波峰对波谷，正负相消，总和（$\mathcal{R}$）就会趋近于 0。$\mathcal{R}(\tau)$ 的值越大，说明 $\tau$ 这个周期越“真实”，我们越应该信赖在这个滞后长度下的历史信息。

**==使用方式==**： 当计算出所有可能的**$\tau$ **对应的$\mathcal{R}(\tau)$ ，从中只挑选出$\mathcal{R}(\tau)$最大的前**k**（**Top-K**）个$\tau$,  根据$\mathcal{R}(\tau)$的大小，**对找到的历史子序列进行加权**，**置信度越高**，**该周期的信息在预测未来时所占的权重就越大**。

**![image-20251123215451676](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251123215451676.png)**

### Autoformer的核心：时延信息聚合（Time delay aggregation）

为了实现序列级连接，我们需要将相似的子序列信息进行聚合。它使用Roll()操作替代传统的点对点矩阵。

在标准的 Self-Attention 中，我们通过计算 $QK^T$ 来让每一个 Token 去“寻找”其他相关的 Token。 而在 Autoformer 中，既然我们已经计算出了最强的 $k$ 个周期滞后 $\tau_1, \dots, \tau_k$（比如昨天、上周、上个月），我们就需要把这些历史时刻的数据**“搬运”**到当前时刻来进行融合。

**Roll 操作的物理含义：** $Roll(V, \tau)$ 的意思是**将整个序列 $V$ 向右平移 $\tau$ 个时间步**。目的是**把过去的数据搬运到当前的索引位置上**。
$$
\text{AutoCorrelation}(\mathcal{Q,K,V}) = \sum_{i=1}^{k} \text{Roll}(\mathcal{V}, \tau_i) \hat{\mathcal{R}}_{\mathcal{Q,K}}(\tau_i)
$$
基于**选择的周期性最强的k个进行聚合**，将所有平移并加权后的序列叠加起来，得到最终的输出。**==实际上是在做多周期的特征融合==**。









## ITransformer: Inverted Transformers Are Effective for Time Series Forecasting

![image-20251120135706722](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251120135706722.png)

**解释典型的Transformer架构用于时序预测效果差的原因**，它们的观点：

![image-20251120142410230](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251120142410230.png)

由单个时间步长形成的**Token**，由于过于**局部的感受野**和**同时时间点表示的时间不对齐事件**而难以显示有益信息。

**时间序列预测模型的标准设定：**

输入 (历史窗) $X$：包含$T$个时间步，$N$个变量。矩阵维度为$R^{T*N}$;

输出（预测窗）$Y$:  预测未来$S$个时间步，同样包含$N$个变量, 矩阵维度为$R^{S*N}$

**数据处理的两种视角：**

（1）$X_{t,:}$:  $t$ 时刻所有变量的快照。表示的是Excel表格中的一行，代表“此时此刻所有传感器的读数”。大多数Transformer（如Informer）是将这个$X_{t:}$, 作为一个$Token$输入模型。

（2）$X_{:,n}$: 第$n$个变量的完整时间序列，Excel中的一列，代表 ”**某个传感器在过去一段时间内的所有读数**“。

**为什么否定$X_{t,:}$?**

直接处理$X_{t,:}$(即把同一时刻的不同变量揉在一起)在物理意义上的两个缺点：

（1）系统性时滞（Systematical Time Lags）：在现实世界中，因果关系往往不是瞬时的，如果强制模型只看$X_{t:}$(同时关注$t$时刻的A和B)，可能变量在该时刻并不具备直接因果关系的数据点，真正的关联式错位的。

（2）物理量纲与分布的差异：同一时刻的$X_{t,:}$包含了性质完全不同的数据。这些数据的语义空间（Semantic Space）完全不同。虽然可以归一化，但是在深度学习，将这些物理意义极不相同的数值映射到同一个特征空间进行交互，难以学习到鲁棒的特征。

**为什么拥抱$X_{:n}$?**

单个变量的整条序列具有物理一致性，该变量在$t$时刻和$t+1$时刻的物理性质不变，它们的统计分布是平稳的。将$X_{:n}$视为一个Token进行Embedding，模型更容易学习到特征。



### 层归一化：

在此前Transformer中，层归一化将同一时刻的的**多个变量进行归一化**，使**==每个变量杂糅无法区分==**，提高了注意力建模词关联的难度。一旦收集到的数据没有按时间对齐，**该操作还将引入延迟过程之间的噪声干扰。**

在倒置版本中，**==层归一化作用于Variate Token内部==**，让所有变量的特征都**==处于相对统一的分布下，减弱测量单位的差异==**。这种方式还可以有效处理时间序列的非平稳问题问题。



### 前馈网络：

基于多层感知机的万能表示定理，**==前馈网络作用在整条序列上==**，能够**提取序列的内在属性，例如幅值，周期性，频率谱**（傅立叶变换可视作在序列上的全连接映射），从而提高在其他的序列上的泛化性。



在原始的Transformer中，**模型的预测效果不一定随着输入的历史观测的变长而提升**，在使用倒置框架之后，模型随着历史观测长度的增加，呈现明显的预测误差降低趋势。



## Timer : Transformers for Time Series Analysis at Scale

![image-20251117234744502](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251117234744502.png)



**名词**：大型时间序列模型 (**large time series models, LTSM**), 单序列序列 (**single-series sequence, S3**),  分位数 (**Quantile**)

### 论文解决的问题：

这篇论文试图解决的问题是在时间序列分析领域中，深度学习模型在**小样本场景下性能瓶颈**的问题:

1. **构建统一时间序列数据集（Unified Time Series Dataset，UTSD）**：为了支持大型时间序列模型（LTSM）的研究，论文首先从公开可用的时间序列数据集中筛选和构建了一个具有层次化能力的统一时间序列数据集（UTSD）。这个数据集包含了来自不同领域的大量时间序列数据，为模型提供了丰富的预训练信息。
2. **提出单序列序列（S3）格式**：为了将异构时间序列数据统一处理，论文提出了S3格式，将不同时间序列转换为统一的token序列。这种格式保留了序列的模式，同时允许模型在预训练过程中观察到来自不同数据集和时间段的序列，增加了预训练的难度，迫使模型更加关注时间变化。
3. **采用GPT风格的预训练目标**：论文采用了类似于大型语言模型（如GPT）的预训练目标，即通过自回归下一个时间点预测来训练模型。这种目标允许模型学习时间序列的生成过程，从而在下游任务中展现出更好的泛化能力。
4. **开发时间序列Transformer（Timer）**：基于上述数据集、训练策略和模型架构，论文提出了Timer，这是一个大规模预训练的时间序列Transformer。**Timer采用了与大型语言模型相似的解码器结构，通过自回归生成进行预训练，这使得它在各种时间序列分析任务中展现出显著的泛化能力、可扩展性和适用性。**
5. **统一生成式方法处理下游任务**：为了利用Timer处理不同的时间序列分析任务，论文**将预测、插补和异常检测**等任务统一为一个生成式任务。这样，Timer可以通过微调来适应不同的下游任务，而不需要为每个任务单独训练模型。
6. **评估和分析**：论文通过在**多个真实世界数据集上进行实验**，验证了**Timer在时间序列预测、插补和异常检测等任务中的有效性**。同时，论文还分析了模型的可扩展性，包括模型大小和数据规模对性能的影响，以及不同架构对LTSMs的适用性。



### 模型微调（全量微调）

（1）环境配置：

```bash
conda activate Sundial
pip install -r requirements.txt
```

（2）准备数据集：

将下游数据集放在./dataset/文件夹下，可以使用提供的数据集微调, 也可以使用自定义的数据集进行微调，不过默认是按照7:1:2划分数据集。

（3）下载预训练模型：

将下载的预训练checkpoint放在./checkpoints文件夹下

（4）配置微调脚本：

参考scripts/forecast/Traffic.sh创建微调脚本, 利用关键参数配置：指定预测任务和特征模式（单变量还是多变量等）

（5）执行微调脚本：

```bash
bash ./scripts/forecast/ECL.sh
```

 （6）微调的过程：

- **加载数据**:加载训练、验证和测试数据集
- **初始化优化器**:使用Adam优化器
- **训练循环**:迭代训练指定的epoch数
- **验证和早停**:每个epoch后验证并检查是否早停
- **加载最佳模型**:训练结束后加载验证集上表现最好的模型

（7）自定义数据集微调：

对于自定义数据集，使用`CIDatasetBenchmark`或`CIAutoRegressionDatasetBenchmark`数据加载器，其中`CIDatasetBenchmark`:用于直接多步预测，`CIAutoRegressionDatasetBenchmark`:用于迭代多步预测(需要`label_len`参数)

#### 微调技巧

1. **数据稀缺实验**:可以通过`subset_rand_ratio`参数控制使用的训练数据比例
2. **学习率调整**:建议使用较小的学习率(如3e-5)进行微调
3. **批量大小**:根据GPU内存调整batch_size
4. **IMS模式**:使用`--use_ims`启用迭代多步预测模式， 其中--use_ims参数表示可以按迭代多步方式评估Decoder-Only模型，未启用则按照直接多步方法评估Encoder-Only模型，**Direct Multi-step (DMS)** and **Iterative Multi-step (IMS)** 



### 代码分析

#### S3格式的处理

（1）数据的处理：使用border1s和border2s来定义训练集、验证机、测试集的起止位置；ETTH、ETTm用固定的分割方式分割数据集。

```python
        if self.data_type == 'custom':
            data_len = len(df_raw)     # 训练, 验证, 测试集划分为70%, 20%, 10%
            num_train = int(data_len * 0.7)
            num_test = int(data_len * 0.2)
            num_vali = data_len - num_train - num_test
            border1s = [0, num_train - self.input_len, data_len - num_test - self.input_len]
            border2s = [num_train, num_train + num_vali, data_len]  # border1s和border2s用于定义训练集、验证集和测试集的起止位置
```



Pytorch中的DataLoader需要通过**一维索引**来访问样本, 需要一个映射机制将**一维索引**转换为**二维坐标**

（2）**S3的思想：将每个变量视为独立的单变量时序**，每次使用__getitem__调用只返回一个变量的时序片段, 而不是所有变量, 利用c_begin: c_begin+1来选择一列（一个变量）

① 计算每个变量能产生多少个窗口

② 数据集的组织

> 索引范围          对应内容   
>
> [0, 880]         变量0的881个窗口   
>
> [881, 1761]      变量1的881个窗口     
>
> [1762, 2642]     变量2的881个窗口   
>
> [2643, 3523]     变量3的881个窗口   
>
> [3524, 4404]     变量4的881个窗口   
>
> [4405, 5285]     变量5的881个窗口   
>
> [5286, 6166]     变量6的881个窗口

③ 索引到变量和时间的映射

**整除运算**：`c_begin = index // n_timepoint` 确定变量编号

- `index = 0 到 880`: `0 // 881 = 0` → 变量0
- `index = 881 到 1761`: `881 // 881 = 1` → 变量1
- `index = 1762 到 2642`: `1762 // 881 = 2` → 变量2

**取模运算** : `s_begin = index % n_timepoint` 确定该变量内的时间起点:

- `index = 0`: `0 % 881 = 0` → 变量0的第0个窗口
- `index = 100`: `100 % 881 = 100` → 变量0的第100个窗口
- `index = 881`: `881 % 881 = 0` → 变量1的第0个窗口
- `index = 1000`: `1000 % 881 = 119` → 变量1的第119个窗口

④ 训练集的稀疏采样：通过将**索引乘以采样间隔**实现跳跃式的选择样本

当 `index = index * self.internal` 执行时,实际访问的样本索引会被放大。

**具体示例**(假设 `subset_rand_ratio = 0.2`,`internal = 5`):

| DataLoader 给的 index | 乘以 internal 后 | 实际访问的样本 |
| --------------------- | ---------------- | -------------- |
| 0                     | 0 × 5 = 0        | 第0个样本      |
| 1                     | 1 × 5 = 5        | 第5个样本      |
| 2                     | 2 × 5 = 10       | 第10个样本     |
| 3                     | 3 × 5 = 15       | 第15个样本     |
| ...                   | ...              | ...            |

能够实现每隔5个样本取1个的效果，同时__len__方法返回的长度也适配了训练过程的采样，能够覆盖原始数据集中均匀分布的样本。

假设:

- 原始数据集有 6167 个样本 (7个变量 × 881个时间窗口)
- `subset_rand_ratio = 0.2`  设置 `--subset_rand_ratio`参数来决定小样本场景下训练样本的占比。
- `internal = 5`

**训练集的处理**:

1. `__len__()` 返回 `int(6167 × 0.2) = 1233`
2. DataLoader 会请求索引 0, 1, 2, ..., 1232
3. 在__getitem__中:
   - 请求 index=0 → 实际访问 0×5=0
   - 请求 index=1 → 实际访问 1×5=5
   - 请求 index=2 → 实际访问 2×5=10
   - ...
   - 请求 index=1232 → 实际访问 1232×5=6160

这样就从 6167 个样本中均匀采样了约 20% 的样本。



#### `CIAutoRegressionDatasetBenchmark`与`CIDatasetBenchmark`两个数据集类的区别

(1) `CIDatasetBenchmark`是直接预测模式， 关键在于`__getitem__`：

```python
    def __getitem__(self, index):
        if self.set_type == 0:
            index = index * self.internal
        c_begin = index // self.n_timepoint  # select variable 选择变量
        s_begin = index % self.n_timepoint   # select start time 选择时间起点
        s_end = s_begin + self.input_len
        r_begin = s_end
        r_end = r_begin + self.pred_len
        seq_x = self.data_x[s_begin:s_end, c_begin:c_begin + 1]  # 提取单个变量的时序片段
        seq_y = self.data_y[r_begin:r_end, c_begin:c_begin + 1]  # 提取单个变量的预测目标
        seq_x_mark = self.data_stamp[s_begin:s_end]  # 输入序列的时间特征
        seq_y_mark = self.data_stamp[r_begin:r_end]  # 预测目标的时间特征

        return seq_x, seq_y, seq_x_mark, seq_y_mark
```

> 预测窗口紧接在输入窗口之后，没有重叠
>
> 目标序列长度为：`pred_len`
>
> `seq_y`形状：`[pred_len, 1]`, 只包含需要预测的未来值

```
时间轴: [0 -------- 672] [672 -------- 768]  
         ↑ seq_x (输入) ↑  ↑ seq_y (目标) ↑  
         s_begin    s_end  r_begin    r_end  
```

- `seq_x`: 时间步 0-672 (输入序列)
- `seq_y`: 时间步 672-768 (预测目标,长度96)
- **无重叠**,直接预测未来96步

(2) `CIAutoRegressionDatasetBenchmark`是自回归预测模式

```python
    def __getitem__(self, index):
        if self.set_type == 0:
            index = index * self.internal
        c_begin = index // self.n_timepoint  # select variable
        s_begin = index % self.n_timepoint   # select start time
        s_end = s_begin + self.input_len
        r_begin = s_end - self.label_len
        r_end = r_begin + self.label_len + self.pred_len
        seq_x = self.data_x[s_begin:s_end, c_begin:c_begin + 1]
        seq_y = self.data_y[r_begin:r_end, c_begin:c_begin + 1]
        seq_x_mark = self.data_stamp[s_begin:s_end]
        seq_y_mark = self.data_stamp[r_begin:r_end]
        return seq_x, seq_y, seq_x_mark, seq_y_mark
```

> 预测窗口**向前延伸** `label_len`,与输入窗口有重叠
>
> 目标序列长度为 `label_len + pred_len`
>
> `seq_y` 形状: `[label_len + pred_len, 1]` - 包含历史标签和未来预测值

```
时间轴: [0 -------- 672]  
                [96 ------------ 768]  
         ↑ seq_x ↑  ↑   seq_y      ↑  
         s_begin s_end r_begin  r_end  
                      (s_end-576)  
```

- `seq_x`: 时间步 0-672 (输入序列)
- `seq_y`: 时间步 96-768 (标签+预测,长度672)
  - 前576步 (96-672): 历史标签,与 `seq_x` 末尾重叠
  - 后96步 (672-768): 需要预测的未来值
- **有重叠**, 自回归预测



#### PatchEmbedding分析

`PatchEmbedding` 将连续的时间序列分割成固定长度的 patches,然后将每个 patch 投影到高维嵌入空间

```python
class PatchEmbedding(nn.Module):
    def __init__(self, d_model, patch_len, stride, padding, dropout, position_embedding=True):
        super(PatchEmbedding, self).__init__()
        # Patching
        self.patch_len = patch_len
        self.stride = stride
        self.padding_patch_layer = nn.ReplicationPad1d((0, padding))  # 在末尾使用复制填充

        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space
        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)
        self.positioned = position_embedding

        # Positional embedding
        if position_embedding:
            self.position_embedding = PositionalEmbedding(d_model)

        # Residual dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # do patching
        n_vars = x.shape[1] # [B, M, T]
        x = self.padding_patch_layer(x)
        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride) # [B, M, N, L]
        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))
        # Input encoding
        if self.positioned:
            x = self.value_embedding(x) + self.position_embedding(x)
        else:
            x = self.value_embedding(x)
        return self.dropout(x), n_vars
```

- **`patch_len`**: 每个 patch 的长度(时间步数)
- **`stride`**: patch 之间的步长,在 Timer 中等于 `patch_len`(非重叠)
- **`padding`**: 填充长度,确保序列可以被完整分割
- **`d_model`**: 嵌入维度-1024
- **`position_embedding`**: 是否使用位置编码

> x.unfold(dimension=-1, size=self.patch_len, step=self.stride)

沿着最后一维创建滑动窗口，Timer在输入之前，对x=[551, 672, 1]做了交换维度的操作, permute之后变成了[551, 1, 672]；每个窗口patch的大小是96, 步长也是 96。patchembedding之后维度是[551, 1, 7, 96]，之后将batch和变量维度合并，这样做是因为Timer能够对**每个变量独立处理, 将他们视为独立的样本**。

`PatchEmbedding` 通过以下步骤将时间序列转换为 patch 嵌入:

1. **填充**: 确保序列长度可被 `patch_len` 整除
2. **分割**: 使用 `unfold` 将序列分割成非重叠的 patches
3. **重塑**: 将 batch 和变量维度合并,独立处理每个变量
4. **投影**: 通过线性层将每个 patch 投影到高维空间
5. **位置编码**: 添加位置信息(可选)
6. **Dropout**: 正则化

#### Timer执行的完整流程

1. 输入归一化: `[B, L, M]` → `[B, M, L]`
2. Patch embedding: `[B, M, L]` → `[B*M, N, D]`
3. Transformer 处理: `[B*M, N, D]` → `[B*M, N, D]`
4. 投影回时间域: `[B*M, N, D]` → `[B*M, N, patch_len]`
5. 重塑和反归一化: `[B*M, N, patch_len]` → `[B, L, M]`





## TimeMOE:Billion-Scale Time Series Foundation Models with Mixture of Experts

![image-20251117234824132](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251117234824132.png)

### 论文的关键创新：

1. 强大的混合专家架构：Time-MoE采用稀疏激活机制，在预测任务中仅激活部分网络节点，这不仅确保了高预测精度，还显著降低了计算负担，完美解决了时序大模型在推理阶段的计算瓶颈。

2. 灵活的预测范围：**Time-MoE**支持**任意长度的输入和输出范围**，能够处理**从短期到长期的各种时序预测任务**，实现了真正的**全域时序预测**。

3. 全球最大规模的**开源时序数据集**：团队开发了Time-300B数据集，涵盖9个领域的超过3000亿个时间点，为模型提供了丰富的多领域训练数据，确保其在多种任务中的卓越泛化能力。

### 模型架构：

1、输入的Token Embedding：使用Point-Wise Tokenization来确保时间信息的完整性

“Point-wise” 是“逐点的”或“按点的”。在自然语言处理中，Tokenization（分词）通常指将一段文本切割成更小的单元，如词语、子词或字符。**“Point-wise Tokenization” 可以理解为将时间序列中的每一个单独的数据点视为一个Token。**

使用Point-Wise Tokenization之后，针对每个token，会利用SwiGLU将其映射到一个高维的向量空间

2、采用MOE架构，使用多个专家替代单一的FFN层，并设计了一个共享专家捕获公共知识，其他多个专家按需激活和学习，同时增加一个辅助损失函数，让专家模型实现负载均衡。

3、Time-MoE设计了一种**多分辨率预测头**，可以同时进行不同尺度的预测，突破了单一尺度预测的局限。在训练时，不同分辨率头会被联合优化。在与推理时，模型采用**==贪心算法==**，利用不同尺度的输出组合成任意的预测长度。这种设计**允许模型根据不同的预测范围进行灵活预测**，并在训练过程中综合**多个预测尺度的误差来优化模型的泛化能力**，从而显著提升预测的准确性和鲁棒性。

4、损失函数：不仅考虑到多分辨率预测头的自回归损失，也考虑到专家模型的辅助损失函数。



### 多分辨率预测头详细解释：

多分辨率预测模块，==允许模型同时学习多个不同长度的预测范围==。对于预测头$p_j$, 用于预测后续$p_j$个时间步
$$
X_{t+1, t+p_j}= W_{p_j}h^{L}_{t}
$$


根据可学习的权重矩阵$W_{p_j}$、Transformer模块最后一层输出$h^{L}_{t}$得到最后的预测结果。

在训练阶段，所有投影同时优化，意味着模型通过共享隐藏状态，并行学习不同时间范围的预测模式，这提高了模型的表达能力和效率。

在推理阶段，使用贪婪调度算法，目标是根据任意给定的输出长度$H$生成预测长度, 由于模型在训练时值学习了固定数量的投影，不能直接输出任意长度，需要一种调度算法来组合这些投影。

**贪婪调度算法**：在自回归预测过程中，在每一步预测中，模型需要选择一个合适的投影$p_j$来生成一段预测序列，然后基于预测结果继续生成后续部分，直到总长度达到预测长度$H$。算法通过贪婪算法选择投影，确保每一步都尽可能高效地覆盖剩余长度。



### 代码分析:

**时序数据的Embedding层**:  TimeMoEInputEmbedding类

​	不同于**NLP的Embedding过程**, 查询表中的词向量, 对于时序数据, 这里使用 `nn.Linear` 将连续的时间序列数值（维度 `input_size`，通常为 1）投影到 `hidden_size`

​	NLP的Embedding过程: 在pytorch框架中, `nn.Embedding(vocab_size, hidden_size)` 在内部维护了一个巨大的矩阵（权重矩阵），形状是 `[词表大小, 向量维度]`; 它的流程是:

​	(1) Tokenization (分词): 对于输入文本`"I love AI"`, 分词器将其转换为整数索引 (Indices): `[42, 7, 1024]`, 这里列表中存放的是整数ID, 不是向量.

​	(2) Embedding (查表): 模型根据这些整数ID去Embedding矩阵中查找对应的"行", 得到向量.

**时序数据的旋转位置编码层**: TimeMoERotryEmbedding类

问题一: RoPE控制旋转位置编码角度的是什么?

​	**控制旋转位置编码角度**的核心参数是**频率基数** (**base**) 和**维度索引** (**dimension index**)

​	频率基数(base): 

​	① 基础数学公式: $inv\_freq = 1.0 / (base ** (torch.arrage(0, dim, 2).float() / dim))$

​	② 拆分之后: $i = torch.arrange(0, 512, 2)$ , $exponent = i / dim$, $inv\_freq = 1.0 / (base ** exponent)$

​	③ base对频率的影响, 一般base=10000, 如果base等于1000时, 它的频率衰减慢,高频分量多,适用于精细位置感知的任务; 当base=100000时 ,频率衰减快, 低频分分量占主导地位,适合处理超长序列。

![image-20251119204032542](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251119204032542.png)

​	维度索引控制: $torch.arange(0, self.dim, 2) / self.dim$

​	① **低纬度索引** (靠近0) : 旋转速度较慢, 获取长期依赖

​	② **高纬度索引** (靠近dim): 旋转速度较快, 捕获局部模式

```python
# 原始代码
i = torch.arange(0, self.dim, 2)  # [0, 2, 4, 6, ..., dim-2]
exponent = i / self.dim           # [0, 2/dim, 4/dim, ..., (dim-2)/dim]
inv_freq = 1.0 / (self.base ** exponent)

# 这实际上是对维度进行归一化，确保指数在[0, 1]范围内
# 对于dim=512：
i = [0, 2, 4, ..., 510]
exponent = [0, 0.0039, 0.0078, ..., 0.9961]  # 均匀分布在[0,1]区间


def analyze_frequency_distribution(dim=512, base=10000):
    i = torch.arange(0, dim, 2)
    exponents = i.float() / dim
    frequencies = 1.0 / (base ** exponents)
    
    print("维度索引分析:")
    print(f"总维度对数量: {len(i)}")
    print(f"最低维度对 (i=0): 频率 = {frequencies[0]:.4f}")
    print(f"中间维度对 (i={dim//4}): 频率 = {frequencies[dim//8]:.4f}")
    print(f"最高维度对 (i={dim-2}): 频率 = {frequencies[-1]:.4f}")
    
    return frequencies

freqs = analyze_frequency_distribution()


dim = 8
base = 10000
i = torch.arange(0, dim, 2)  # [0, 2, 4, 6]
exponents = i / dim          # [0.0, 0.25, 0.5, 0.75]
inv_freq = 1.0 / (base ** exponents)

print("维度索引控制示例 (dim=8):")
for idx, (dim_idx, exp, freq) in enumerate(zip(i, exponents, inv_freq)):
    period = 2 * 3.14159 / freq  # 旋转周期
    print(f"维度对{idx}: 索引={dim_idx}, 指数={exp:.2f}, 频率={freq:.4f}, 周期≈{period:.1f}位置")

# 输出结果:  
# 维度对0: 索引=0, 指数=0.00, 频率=1.0000, 周期≈6.3位置
# 维度对1: 索引=2, 指数=0.25, 频率=0.1000, 周期≈62.8位置  
# 维度对2: 索引=4, 指数=0.50, 频率=0.0100, 周期≈628.3位置
# 维度对3: 索引=6, 指数=0.75, 频率=0.0010, 周期≈6283.2位置

"""
低维度角色：
- 旋转速度快：每6-7个位置完成一次完整旋转
- 擅长捕获：局部模式、语法结构、短程依赖
- 类比：高音喇叭，敏感但传播距离短
"""

"""
高维度角色：
- 旋转速度慢：需要数千位置才完成一次旋转
- 擅长捕获：语义主题、文档结构、长程依赖
- 类比：低音炮，传播距离远但不够敏感
"""

"""
中层维度作用：
- 连接高低维度的频率间隙
- 处理中等距离的依赖关系
- 确保频率分布的连续性
"""
```



**时序数据的Attention层**: TimeMoEAttention类

​	实现了带有 **RoPE (Rotary Positional Embeddings)** 的自注意力机制

​		

**时序数据的预测层**: TimeMoeForPrediction类

​	包含多个 `TimeMoeOutputLayer`，对应不同的 `horizon_lengths`。这意味着模型可以同时被训练来预测未来 1 步、12 步或 96 步等。

​	**损失函数**: ① **预测损失**: 使用 **`HuberLoss`** (比 MSE 对异常值更鲁棒). ② **负载均衡损失 (Aux Loss)**: `load_balancing_loss_func`，防止 Router 总是选择同一个专家，确保专家负载均衡。



**数据的生成逻辑**: TSGenerationMixin类

​	在NLP的大语言模型中，生成的每一步都是从词表（Vocabulary）中选择一个概率最大的TokenID (整数)。而在**TimeMOE**中，**生成的每一步是预测未来的数值**（浮点数），而且**可能一次预测未来多个时间步**（Multi-step horizon）。

​	`TSGenrationMixin`的核心作用就是**重写（Override）**，Hugging Face `GenerationMixin`中的搜索策略，使其不再执行“分类任务”（选词），而是执行“回归任务”（预测数值）。

​	`TSGenrationMixin`这个类主要重写了`_greedy_search`方法，这是最基础的生成策略，即每一步都取模型输出的最优解。

​	**重写逻辑如下**：

​	（1）输入数据的类型处理

​		① 原始NLP：期望输入`input_ids`是`LongTensor`（整数ID），代表词汇索引。

​		② 重写版（Time-MOE）: 虽然参数名仍叫`input_ids`, 但实际上它处理的是`FloatTensor`（连续数值）。

​	（2）取消`argmax`操作（**关键修改**）

​		① 原始NLP：

```python
# NLP 逻辑：从 Logits 中选概率最大的词 ID
next_tokens = torch.argmax(next_token_logits, dim=-1)
```

​		② 重写版 (Time-MOE):

```python
# Time-MoE 逻辑：Logits 本身就是预测的数值
next_tokens = next_tokens_scores  # 直接使用模型输出
```

​	注意：Time-MOE的输出层直接是一个线性层，输出的就是预测的数值，不需要通过Softmax归一化，也不需要`argmax`选取索引。

​	（3）动态Horizon支持（多步预测）

​		① Time-MOE支持一次预测未来多个时间点（例如一次预测未来96个点）

```python
# 形状调整：[batch_size, horizon_length * input_size] -> [batch_size, horizon_length, input_size]
next_tokens = next_tokens.reshape(batch_size, -1, self.config.input_size)
horizon_length = next_tokens.shape[1]

# 拼接：将预测出的 1 个或 N 个点拼接到输入序列末尾
input_ids = torch.cat([input_ids, next_tokens], dim=-2)
```

​		② 允许模型在每一步生成中不仅仅推进一个单位时间，而是推进`horizon_length`个单位长度

​	（4）停止条件

​		① 保留基于最大长度（max_length）的停止机制

​		② 当序列长度超过设定的`max_length`时，循环终止

​	**重写之后，它是如何被调用的？**

​	`TSGenerationMixin` 并不是一个独立运行的脚本，它是一个 **Mixin（混入类）**，被集成在主模型类中。

​	主模型类`TimeMoeForPrediction`继承这个Mixin, 这使得`TimeMoeForPrediction`的实例自动拥有了`generate`方法及其定制化的`_greedy_search`实现。当用户在推理或评估阶段调用`model.generate()`时触发。



**MoE核心**：TimeMoeSparseExpertsLayer类

① 利用MOE替换了标准的MLP层

② 路由（Router/Gate）: 一个线性层用于计算输入Token对每个专家的权重，取Top-K。

③ 专家（Experts）: `nn.ModuleList`包含多个`TimeMoETemporalBlock`（即MLP）

④ 共享专家（Shared Experts）：所有Token都会**经过共享专家**，用于**捕获通用知识**并提升稳定性。







## Timer-XL: Long-Context Transformers For Unified Time Series Forecasting

![image-20251119154256974](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251119154256974.png)



### 论文的关键创新:

1. 多变量的下一个Token预测和统一的时间预测, 相比其他时序大模型, 考虑到多变量之间的依赖关系, 这使得模型具有更完备的上下文信息
2. 针对多变量依赖关系,提出了TimeAtttention机制, 是一种为多维时间序列量身定制的新型因果自注意力机制，促进了具有位置意识的序列内和序列间建模



### TimeAttention详细解释:

#### 下一词预测:

下一词预测（Next Token Prediction）是大语言模型的主流训练目标。其核心在于训练时**并行优化在多个位置上**的自回归预测信号，推理时，模型可基于**不同的上下文长度**进行预测。本工作将该范式首次扩展到多变量时间序列

![image-20251119163601661](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251119163601661.png)

#### 多变量下一词预测:

基于分块（Patching）后的二维时间序列单元，每个位置的下一词预测不仅依赖于该序列的历史变化（时序因果性），还依赖于相关变量的外生关联.

![image-20251119163826649](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251119163826649.png)

#### Timer的核心:

通过**克罗内克积（Kronecker Product）**将“**变量间的空间依赖**”与“**时间步上的因果依赖**”解耦并重组。

![image-20251119170005137](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251119170005137.png)

![image-20251119164542978](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251119164542978.png)

TimeAttention捕获patch之间的依赖性, 按照时间顺序将二维时间序列展平为一维时间序列.

**原问题**: 针对协变量:它的矩阵 $\mathcal{C}$是 $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$，而不是 `[[1,1]]`? 

**问题改写**: 如果我们只预测 A，只需要知道 A 依赖 A 和 B 及其历史就行了，为什么还要管 B 依赖谁?

**原因分析**: 

​	(1) **Self-Attention 的输入形状**： 模型输入的 Token 序列不仅包含 Target A 的 Patch，也包含 Covariate B 的 Patch。假设我们有 2 个变量（A 和 B），每个变量切成 $T$ 个 Patch，那么输入到 Attention 层的序列总长度是 $2T$。 Attention Map 的大小必须是 $(2T) \times (2T)$。这就要求变量依赖矩阵 $\mathcal{C}$ 必须是 $2 \times 2$ 的，才能通过克罗内克积扩展成完整的大小。

​	(2) 矩阵的每一行代表“谁在看”：$\mathcal{C} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ 这个矩阵要按行来解读：

​	**第一行 (Row 1, Variable A)**: $[1, 1]$。

​		这意味着：变量 A 的 Token (Query) 可以去 Attention 变量 A (Key) 和 变量 B (Key)。

​		**物理意义**：A 是我们要预测的目标，它既受自身历史影响，也受协变量 B 的影响。这符合直觉。

​	**第二行 (Row 2, Variable B)**: $[0, 1]$。

​		这意味着：变量 B 的 Token (Query) **只能**去 Attention 变量 B (Key)，**不能**看变量 A。

​		**物理意义**：这是“协变量”定义的关键。**协变量通常被视为“外生变量” (Exogenous Variable)**，比如天气、节假日。

​		如果 $\mathcal{C}_{2,1} = 1$（即 B 依赖 A），那就意味着 **A 的变化会导致 B 的变化**（例如：商场销量 A 变了，导致天气 B 变了）。这显然是不符合逻辑的。

​		因此，为了保持 B 的纯粹性（它只按自己的规律演变，不受目标变量 A 的干扰），我们必须切断 A -> B 的注意力路径。这就是那个 `0` 的由来。

**原问题**: 为什么协变量和多变量的矩阵不一样？实际依赖关系不应该一样吗？

**原因分析**: 在时间序列预测的建模中，**“多变量预测” (Multivariate) 和 “带协变量的单变量预测” (Univariate with Covariate) 是两种完全不同的任务假设。**

**(1) 多变量 (Multivariate) 场景**

​	**任务定义**：同时预测 A 和 B。我们认为 A 和 B 是**耦合系统 (Coupled System)**。

​	**依赖假设**：互为因果。比如：A 是“高速公路流量”，B 是“平均车速”。流量大了车速会慢（A -> B），车速慢了可能导致拥堵反过来影响流量分布（B -> A）。

​	**矩阵体现**：$\mathcal{C} = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$。A 需要看 B 的历史来预测 A。B 也需要看 A 的历史来预测 B。这是一个**双向图**。

**(2) 协变量 (Covariate) 场景**

​	**任务定义**：只预测 A，利用 B 作为辅助信息。我们假设 B 是**驱动因素**。

​	**依赖假设**：单向因果。比如：A 是“冰淇淋销量”，B 是“气温”。气温升高会增加销量（B -> A）。但销量增加**绝对不会**让气温升高（A -x-> B）。

​	**矩阵体现**：$\mathcal{C} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$。A 依赖 B（利用气温预测销量）。B **不**依赖 A（计算气温的特征表示时，不需要看销量）。这是一个**单向图**。





## DGraFormer:  Dynamic Graph Learning Guided Multi-Scale Transformer for Multivariate Time Series Forecasting

![image-20251127172001498](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251127172001498.png)

该论文利用离散傅里叶变换，将原始数据从时域转换到频域，找到TopK的频率，反傅里叶变换到时域，去除噪声，只保留有周期性的信息。利用去噪后的数据构建计算静态的权重矩阵，代表了变量间的普遍关系。

窗口划分：通过将W个时间窗口划分，每个窗口包含m个时间步，为了捕捉动态相关性，针对每个时间窗，随机初始化节点嵌入，并构建一个可学习的权重矩阵，则时间窗的权重矩阵是$E= \alpha C + (1-\alpha)R_w, \alpha \ in (0.1, 0.9)$

最后从权重矩阵$E$选取重要的变量构建一个权重矩阵，掩码去掉无关矩阵。

参考了PatchTST的相关工作，通过扩展单patch到多patch，实现多尺度的研究工作。





## Falcon-TST： A Large-Scale Time Series Foundation Model

**Prediction过程**：

① 输入长度大于**固定长度2880**会截断，小于2880会进行**前置padding**

② **RevIN**处理，并通过**旋转位置编码嵌入相对位置信息**

③ **选定专家路由**

④ **共享专家**

⑤ **Patch Embedding**, 输入 [batch, patch_num, patch_size] --> [patch_num, batch_size, hidden_size],  **输出**：**hidden_states, attention_mask， input_mask**

⑥ **Transformer Layer**: 没使用**因果注意力机制**，采用的Transfomer的Encoder架构

⑦ **共享专家输出+领域专家输出**，其中前**固定长度seq_length**为原始， 后面**336为预测长度**

⑧ **推理出混合专家的输出**，根据自回归头预测，如果混合专家输出长度大于目标长度，截取目标部分即可。

### FalconTST的预测模式：自回归式多步预测（分段式）

① FalconTST在推理时，每一次forward都预测一个固定的最长步长（**max(multi_forecast_head_list)**）；

② 在自回归滚动过程中，并不会每次都使用这96步全部结果，而是根据剩余预测长度，只截取其中前多少步做有效预测，并拼接回输入继续预测。

**==FalconTST采用N-BEATS方法进行迫使模型逐层剥离信号。==**

![image-20260107132259484](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260107132259484.png)

**N-BEATS**模型由**多个Stack模块组成**，**每个stack由多个block串联而成**，其中第一个block的输入为原始输入序列，输出为两部分，一是**对于未来窗口H的预测值$\hat{y}$**, 二是**对于block输入的重构值$\hat{x}$**。一方面，下一个block作为之前block的补充，不断地去拟合之前block没有拟合到的残差信息，另一方面，可以将该过程看作对时间序列的分解，不同的stack中block拟合时间序列某一部分的信息，最终NBEATS的输出为各个stack的输出。

因此，**FalconTST**在使用时，使用`FalconTSTMoELayer`层替换`FC`层结构，其中`FalconTST`的**共享专家和路由专家**都是由`Transformer的Encoder层`构建的，因此利用MOE中共享专家与路由专家的输出，**合并之后分割成backcast和forecast**，不断获取forecast，并求和，backcast不断缩减，直接最后为空。

**==差异点==**：共享专家和路由专家共同构建了是一个完整的基于Transformer架构的模型，它并没有采用两个FC分别建模Backcast和Forecast，而是每一个专家都建模Backcast与Forecast。要求每个专家同时完成：

① **解释过去（Backcast）：** “我认为过去这段序列发生了什么（比如是某个特定频率的波形）”。

② **预测未来（Forecast）：** “基于我对过去的这种理解，我认为未来会怎么走”。

**FalconTST中，`FalconTSTMoELayer`充当一个N-BEATS Block的角色，Block的输入是上一层的残差，Block的输出一个是用于修正的Backcast和一个用于贡献预测的Forecast。**



### 问题点：为什么FalconTST模型使用的是Transformer Encoder还要利用mask机制？

**FalconTST使用Attention Mask不是为了因果性**，主要是为了屏蔽完全被padding填充的无效Patch，确保模型不会关注到没有意义的填充区域，这是数据有效性的过滤。





## ms-Mamba: Multi-scale Mamba for Time-Series Forecasting

![image-20251201113126003](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251201113126003.png)

**关于Transformer模型在时间预测任务中的局限性**

1、注意力机制局限性：**==难以捕捉关键的时间依赖关系==**

虽然Transformer模型最初是为NLP设计的，并因其自注意力机制在捕获长程依赖关系方面的出色表现而被应用于时间序列预测任务，但在处理时间序列数据时，其**基于内容的注意力机制暴露了关键缺陷**：

- **难以检测关键的时间依赖关系**：Transformer的注意力机制是**基于内容（content-based）的**，这意味着它主要根据序列中**不同元素之间的相似性来建立连接，而不是明确地建模时间结构**。
- **依赖关系随时间减弱的问题**：这种**基于内容的机制在处理那些==依赖关系随时间逐渐减弱==**（vanishing correlations over extended horizons）的序列时，表现不佳。
- **强季节性模式问题**：当时间序列数据中**==存在强大的季节性模式==**（strong seasonal patterns）时，Transformer的标准注意力机制也难以有效检测或处理这些模式

2、计算复杂度，**==二次方复杂度限制了长序列处理能力==**

Transformer模型的另一个主要限制是其**自注意力机制的二次方复杂度**

- **复杂度与序列长度的关系**：对于长度为 *L* 的输入序列，**标准的自注意力机制**的计算成本和内存使用量是 *O*($L^2$)，即**与序列长度的平方**成正比。
- **对长序列的影响**：在时间序列预测中，处理长输入序列（long input sequences）以捕获长期模式是很常见的需求，但二次方复杂度极大地**增加了计算成本和内存使用**，成为模型应用的一个限制因素



时间序列预测线性模型发挥的作用：

1、**线性模型的优势：简洁与效率**

线性模型（通常使用多层感知机，MLPs）的优点：它们架构更简单、速度更快，相比于基于Transformer的模型具有更高的效率

2、**线性模型的局限性：缺乏复杂的建模能力**

为了追求速度和简洁性，线性模型牺牲了关键的建模能力：

- **难以处理非线性依赖关系**：这些模型通常难以处理复杂的**非线性依赖关系**（non-linear dependencies）。
- **不适用于复杂模式**：在涉及**高度波动（highly volatile）或非平稳（non-stationary）**模式的场景中，线性模型的性能往往不佳。
- **难以捕捉全局依赖关系**：与Transformer模型相比，线性架构在**捕捉全局依赖关系（global dependencies）**方面效率较低

3、**局限性带来的计算成本悖论**

线性模型缺乏全局依赖关系捕捉能力的后果，反而抵消了其“更快”的优势：

- **对长输入序列的需求**：由于线性架构不能像Transformer那样高效地捕捉全局信息，为了达到可比较的预测性能，它**需要更长的输入序列**（longer input sequences）。
- **计算成本增加**：对更长输入序列的需求，即使模型本身是线性的，也会**增加整体的计算成本**



**多尺度 Mamba 层（Multi-scale Mamba Layer）的结构**

传统的SSMs、Mamba 及其变体（如 S-Mamba）在处理时间序列数据时，通常只使用**一个可学习的采样率** Δ。然而，时间序列数据本质上包含**多个时间尺度**的信号和模式。

**ms-Mamba 的目标**： ms-Mamba 旨在解决这一不足，通过在不同的采样率  下处理输入，从而更好地捕捉和利用时间序列数据的多尺度特性。

**实现机制：** ms-Mamba 是通过**组合多个 Mamba 模块（Mamba blocks）**来实现的，每个模块都配置了不同的采样率 $Δ_i$



对于第 *l* 层的输出嵌入 $E^l$，ms-Mamba 层将其分解并进行并行处理：
$$
E^l_m=Avg(Mamba(E_l;Δ_1),…,Mamba(E_l;Δ_n))
$$
这意味着：

1. 输入嵌入 $E^l$ 被送入 n 个并行的 Mamba 模块。
2. 每个 Mamba 模块使用一个特定的采样率 $Δ_i$​ 进行处理。
3. 最终的输出 $E^l_m$​ 是这 *n* 个并行处理结果的**平均值（Avg）**。
4. 获取不同采样率 $Δ_i$​ 的三种策略

**为了得到用于并行 Mamba 模块的不同采样率 $Δ_i$ ，ms-Mamba 探索了三种不同的策略：**

**策略 1：固定时间尺度（Fixed temporal scales）**

​	在这种方法中，只有基础采样率 Δ1 是可学习的（类似于原始 Mamba 模型）。而其他的采样率 Δ2,Δ3,…,Δ*n* 则通过 Δ1 乘以固定的超参数（hyper-parameters）$α_i$ 来获得：
$$
Δ_i=α_i×Δ_1,i∈\{2,…,n\}
$$
**特点：** $α_i$ 是超参数。通过消融实验发现，系数 *α*=(1,2,4,8) 在不同数据集上表现最佳。这种方法引入了额外的需要调优的超参数，是一个限制。

**策略 2：可学习时间尺度（Learnable temporal scales）**

​	在这种方法中，所有的采样率 $Δ_i$ 都被定义为独立的**可学习变量**，就像原始 Mamba 模型中的 Δ 一样。

**特点：** 这种方法通常能提供略优于最佳固定尺度版本的结果，并且避免了对 $α_i$ 超参数的调优，因此可能是更优选的选项。

**策略 3：动态时间尺度（Dynamic temporal scales）**

​	这是最灵活的方法，其中所有的采样率 $Δ_i$ 都是通过一个**多层感知机（MLP）**根据当前的输入嵌入 $E_l$ 动态估计出来的：
$$
Δ_i=MLP(Flatten(E_l))
$$
**实现细节：**

1. **Flatten(·)**：首先将输入张量 $E_l$（维度为 $L×D_e$​，其中 *L* 是序列长度， $D_e$​ 是嵌入维度）展平为一个维度为 $L*D_e$​ 的向量。
2. **MLP(·)**：该 MLP 由两个线性层组成，中间夹着一个 ReLU 激活函数：$MLP(x)=W_2*max(0,W_1x+b_1)+b_2$​。
3. MLP 的作用是将展平后的输入映射到 *n* 个不同的采样率 Δ*i*​。



## Is Mamba Effective for Time Series Forecasting ?

它提出基于**Transformer模型**在真实场景的时间序列预测中面临的问题：**缺乏处理大规模数据**的**计算效率**和**资源扩展性**

由于Transformer架构的固有二次方计算复杂度，**随着数据量的增加**，其**计算成本呈现爆炸式增长**，导致**它缺乏在资源受限或高实时性要求场景下的生存能力**。

1、**缺乏线性的计算效率**

Transformer的注意力机制在计算时，**其运算量与输入序列长度或变量数量的平方成正比**，这意味着，如果**输入数据的长度或变量数量增加一倍**，**计算量会增加四倍**。在真实场景中，往往**涉及大量的传感器和需要回溯很长的历史数据**，这会导致计算开销急剧增加。相比之下，**线性模型或Mamba**这类状态空间模型（State Space Model, SSM）具有线性复杂度($O(N)$), 运算速度更快。

2、**缺乏低资源占用的轻量化特性**

由于计算复杂度高，Transformer模型在处理长序列时会消耗大量的GPU资源，这会导致模型在推理过程中对硬件设备要求较高。

3、**缺乏处理“高维变量”时的全局捕捉能力**

为了捕捉变量间的相关性，Transformer需要进行全局注意力计算。随着**变量数量的增加**，**计算负载呈现指数级上升**，这使得在变量众多的场景下，使用Transformer变得不切合实际。

总的来说：尽管 **Transformer 在捕捉模式上很强大**，但它**缺乏像 Mamba 或线性模型那样的高效性与低成本特性**。

![image-20251202163646669](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251202163646669.png)

Mamba Block 处理输入序列 *X* (维度为 *B*×*V*×*D*) 的过程可以分为以下几个关键步骤:

1、线性投影与扩展 (Linear Projection)

- 输入数据首先通过线性层进行投影，将隐藏层维度扩展为原来的 *E* 倍（通常 *E*=2）。
- 这一步生成了两个分支：**主分支** *x* 和 **残差门控分支** *z*。

2、卷积与激活 (Convolution & Activation)

- **主分支** *x* 会经过一个一维卷积层 (Conv1D) 和 SiLU 激活函数处理，得到 *x*′。
- **作用：** 这里的小卷积核主要用于捕捉局部的上下文信息，类似于给数据做一个平滑处理或局部特征提取

3、**选择性状态空间模型 (Selective SSM)**

​	传统的 SSM 参数是固定的，而 Mamba 让参数变成**“输入依赖” (Input-Dependent)** 的。

​	（1）**参数生成：** 模型根据当前的输入 *x*′，通过线性层动态计算出三个关键参数：

​		① Δ (步长/时间尺度)

​		② B (输入控制矩阵)

​		③ C (输出控制矩阵)

​	（2）**离散化 (Discretization)：** 利用生成的 Δ，将连续的状态矩阵 *A* 和 *B* 转化为离散形式 (*A*ˉ,*B*ˉ)。这一步让模型能够适应不同采样率的数据,。

​	（3）**状态递归：** 执行核心的递归计算 $h_t=Ah_{t−1}+Bx_t$。

​	**选择性机制的意义：** 因为 Δ, *B*, *C* 是随输入变化的，模型可以根据当前输入的内容，自主决定是**“记住”**这个信息（更新进隐状态$h_t$），还是**“遗忘”**它（重置状态）。这实现了**类似 Transformer 注意力机制的信息筛选功能**。

4、**门控与输出 (Gating & Output)**

经过 SSM 处理后的输出 *y*，会与第一步生成的**门控分支** *z* (经过 SiLU 激活后) 进行逐元素相乘（*y*′=*y*⊗SiLU(*z*)）。

最后，通过**一个线性层将维度映射回原始大小**，得到**最终输出** *Y*。



### Mamba架构相比Transformer架构，计算速度快在哪？

Mamba 基于**状态空间模型**（SSM），它**将历史信息压缩到一个固定的隐状态**（Hidden State）中，无论序列多长，**它在每个时间步的计算量**是固定的。因此，其计算复杂度随序列长度呈线性增长 ($O(N)$)

Mamba 的数学基础是离散化的状态空间模型，递归计算 $h_t=Ah_{t−1}+Bx_t$。**只用计算上一步的隐藏状态和当前输入即可计算下一步的状态**。

Mamba 在生成第 *N*+1 个点时，它不需要看之前的 *N* 个输入，只需要看**上一时刻的一个固定大小的隐状态（Hidden State）** *h**t*。 无论历史序列长度是 100 还是 10000，Mamba 生成下一步的计算量永远是**常数级** *O*(1)。



## Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach

![image-20251204150847651](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251204150847651.png)







## MOMENT: A Family of Open Time-series Foundation Models

![image-20251218121121927](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20251218121121927.png)

基于**Encoder-Only实现的预训练模型**，在大量的**时序数据上进行预训练**。





# MPTSNet: Integrating Multiscale Periodic Local Patterns and Global Dependencies for Multivariate Time Series Classification

![image-20260104135111146](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260104135111146.png)

**提出痛点**：目前基于深度学习的方法往往忽略了在**==不同时间尺度上同时构建局部特征和全局依赖关系==**，缺乏足够的特征提取能力来达到令人满意的分类精度



**多元时间序列分类两个主要的原因**：

（1）时间序列数据本身具有周期性特征，在不同时期呈现相似的趋势，从而导致数据冗余。同时，隐藏的信息在多个时期重叠和相互作用，使时间序列数据的探索复杂化。

（2）在MTS数据中，不同变量之间的**时间序列子序列**在**不同的周期尺度上表现出不同程度的相关，甚至相反的相关**。

现存模型的假设：**不同变量之间的相关性在不同的时间分辨率下保持不变**，这种**假设常常导致变量在不同周期尺度上的两两关系表现不充分**。



## 多尺度周期性

**MPTSNet**首先**使用快速傅里叶变换（FFT）将时域数据转换为频域表示**，**每个表示对应一个特定的周期尺度**，**从而有效地处理数据的多周期性特征并减少数据冗余**。将原始时间序列分解为多个周期分量，选择**Topk**个主要的频率，对应于**Topk**个主要的周期性。



## PeriodicBlock：局部与全局特征的集成

**==基于Inception的局部提取器==**：受到**Inception架构**的启发，该模块使用不同内核大小（如 1、3、5、7、9、11）的多尺度一维卷积层，旨在捕获**周期内（intra-period）**的细微局部模式。

==**基于注意力的全局捕捉器**==：**局部特征提取后**，模型**利用多头注意力机制建模周期之间的全局依赖关系**，**弥补传统CNN在捕捉长程依赖方面的不足**，并能适应多尺度周期的不同时间步。





# Improving Time Series Forecasting via Instance-aware Post-hoc Revision

![image-20260105113304012](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260105113304012.png)

## PIR框架

**PIR框架**的整体思路是**==后处理==**，即**先用一个主干模型给出一个初始预测结果，然后再对这个结果进行修正**。

**目的**：为了修正这个错误，首先必须精准识别哪些预测是失败的。

**本质**：作者将这个**识别失败**的过程建模为一个“**不确定性估计**”任务。（**不确定这个预测是否有把握，需要给出置信度**）

名词解释：

**不确定性估计任务**的目标**不仅是让模型给出预测结果，还要让模型告诉我们它对这个结果有多大把握**。在传统机器学习或深度学习任务中，模型通常给出一个点预测，比如“明天的气温是 25 度”。而**==不确定性估计==**则**要求模型输出一个范围或概率**，比如“明天的气温在 23 度到 27 度的概率是 95%”，或者“我是瞎猜的，置信度很低”。

不确定性的两个主要来源：

① **数据不确定性**：数据本身就是乱的、有噪声的，这是客观存在的，无法通过增加数据量来消除。（如：传感器故障导致的数据缺失、测量误差等）

② **模型不确定性**：模型因**见识太少或能力不足**而感到困惑，这是主观的，可以通过增加更多训练数据来降低。（如：**长尾分布**）



## Failure Identification: 如何找出预测结果中哪些样本是可靠的

**PIR的解决方案**：**以“误差”作为“不确定性”的代理**，提出一种数据驱动方法：**训练一个小型的神经网络**来**==预测“预测误差本身”==**。

**具体实现**：设计两层的全连接神经网络$f_\theta$用于估计不确定性$\sigma$。

输入特征:  

​	① 原始输入时间序列$x$

​	② 主干生成的初始预测结果$\hat{y}$

​	③ $E$: 通道嵌入矩阵，用于捕获不同变量（Channel）的特性。

输出特征：

​	$\delta$：估计的确定性值

辅助函数：

​	**由于无法直接获得“真实的不确定性”**，作者假设：**预测误差越大，意味着不确定性越高**。因此，模型被训练去预测MSE（均方误差）。

​	公式如下 ：
$$
\mathcal{L}_{ue} = \frac{1}{N}\sum_{1}^{N}||\delta - ||\bar{y} - y||_{2}^{2}||_{1}
$$

- **$||\bar{y} - y||_{2}^{2}$**：这是**真实的预测误差**（即初始预测 $\bar{y}$ 与真实标签 $y$ 之间的 MSE）。
- **$\delta$**：这是网络**估计的不确定性**。
- **$||\cdot||_{1}$**：使用 MAE（L1 Loss）来让 $\delta$ 尽可能逼近真实的 **MSE**。

Failure Identification的作用：

通过这个模块，框架知道主干模型在当前样本上的表现大概率会产生多大的误差（$\delta$）, 如果 $\delta$ 很大（预测很不准），后续的 $\alpha$ 和 $\beta$ 权重就会变大，模型就会更多地依赖 **Local Revising** 和 **Global Revising** 的结果来修正原始预测。



## Local Revising: 局部修正

**==利用局部窗口内的上下文信息来增强预测准确性==**。

① **利用领先-滞后效应**，时间序列数据中，协变量的变化往往领先于目标变量，比如：气温升高（协变量）可能导致几个小时后的用电量增加。因此，协变量的预测结果可以暗示未来趋势。

② **引入先验已知信息**，有些信息是**提前已知的**，比如时间戳、节假日或天气预报。这些被称为外生变量。他们作为先验条件，可以帮助模型应对由自然规律引起的突发分布偏移。

③ **弥补通道独立策略的不足**：很多模型选择通道独立的策略，将每个变量单独作为一条序列，忽略变量之间的关系，Local Revising模块重新引入变量间的关联。



**具体实现**：

**嵌入与投影**： 

模型将协变量和先验信息映射为隐藏状态并拼接到一起$H_0 = [h_{co}, h_{exo}]$ ：

① **中间预测结果 ($\bar{y}$)**：即主干模型输出的初步预测。通过一个可训练的线性投影层 `CoVariateEmb` 转化为协变量表示 $h_{co}$ 。

② **外生变量 ($c$)**：如时间、文本描述等。通过 `ExoVariateEmb` 转化为外生变量表示 $h_{exo}$ ，如果是数值特征就用线性层，如果是文本描述可以使用语言模型处理。

**Transfomer处理与生成**：

① **相关性提取**：将拼接后的 $H_0$ 输入到一个标准的 **Transformer** 模块中。利用 Transformer 的 **Attention 机制**，模型可以显式地捕捉协变量（预测值）与外生变量（如时间、环境因素）之间的复杂关联 。

② **生成修正结果**： 最后通过一个线性预测头（Linear prediction head）输出修正后的局部预测结果。



## Global Revising： 全局修正

**传统模型**通常**在大多数常见样本上表现良好**，但在遇到**稀有或特殊的数值模式时容易失败**，这些**稀有样本构成了所谓的长尾分布**。既然模型没有学好稀有模式，**不如直接检索历史数据库**，**看看以前发生类似情况时，后续走势如何，然后直接照搬或参考以前的走势**。

构建检索数据库：

​	① 数据来源：只使用训练集的输入-输出对（$X_{train}$,$Y_{train}$）构建数据库

检索相似序列**（Top-K Retrieval）**:

​	对于当前的输入序列 $x$，系统会在数据库中寻找最相似的 $K$ 个历史序列: 

​		**编码（Encoding）：使用Enc()函数对序列进行处理**。

​		**相似度计算**：**使用余弦相似度计算输入$x$与数据库中序列的距离**

​		结果：得到最相似的K个历史片段及其对应的未来真值$Y$

生成全局修正结果：

​	**核心假设**：**相似的实例往往表现出相似的未来趋势**，这意味着**检索的历史真值本身就可以直接作为当前预测的参考**。

​	**加权求和**: 系统不修改主干模型结构，而是对检索到的K个历史未来值进行加权平均。



# MDMLP-EIA: Multi-domain Dynamic MLPs with Energy Invariant Attention for Time Series Forecasting

![image-20260106135303550](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260106135303550.png)

## Simple Moving Average（SMA）简单移动平均

**简单移动平均**是一种**通过计算滑动窗口内数据均值来提取时间序列趋势项的方法**。

在Autoformer、Dlinear等模型中，SMA被用于将原始数据分解为趋势（Trend）和季节性（Seasonality）两部分。

SMA公式：
$$
s_{t} = \frac{1}{k} \sum_{i=t}^{t+k-1} x_{i} = \frac{x_{t} + x_{t+1} + ... + x_{t+k-1}}{k}
$$
**参数解释：**

- **$X = \{x_1, x_2, ..., x_n\}$**：原始的时间序列数据点。
- **$k$**：滑动窗口的大小（Kernel size），即每次计算平均值时包含的数据点个数（例如 $k=3$ 或 $k=25$）。
- **$t$**：当前的移动步数（时间步索引）。
- **$s_t$**：计算得到的第 $t$ 个移动平均值。

具体计算过程：

① 填充（Padding）

由于移动平均操作会导致序列长度变短，为了保证输出的趋势序列长度与原始序列一致，必须在序列的两端进行填充 。实现是：重复序列的第一个值填充在头部，重复最后一个值填充在尾部。

② 滑动窗口计算（Moving Average）

假设有一个长度为k的窗口，它在数据上滑动。对于每一个位置，选择窗口内k个数值的无加权平均值。

③ 分解结果

趋势项：$X_T = AvgPool(Padding(X))$

季节性项：$X_S=X-X_T$



## Exponential Moving Average（EMA） 指数移动平均

**指数移动平均**指的是**它对数据点进行加权平均**，**但权重不是相等的**。它给最新的数据点分配更大的权重，而**对较旧的数据点分配较小的权重**，这种**权重随时间呈指数级衰减**。这种机制使得EMA能够更迅速地响应时间序列中潜在趋势的变化，同时平滑掉旧数据的噪音。

具体计算过程：

① 初始化

序列的第一个点的EMA值（趋势值$s_0$）直接等于原始数据中的第一个点$x_0$

② 递归计算

从第二个时间点开始（$t>0$）,每一个新的EMA值$s_t$都是由当前时刻的原始数据$x_t$和上一时刻的EMA值$s_{t-1}$加权混合而成的。
$$
s_t = \alpha x_t + (1 - \alpha)s_{t-1}
$$
**$x_t$**：当前时刻的原始数据输入。

**$s_{t-1}$**：上一时刻计算出的趋势值（包含了过去的历史信息）。

**$\alpha$ (Alpha)**：**平滑因子**，取值范围是 $0 < \alpha < 1$ 。

③ 分解结果

计算整个序列的EMA值之后，将原始时间序列X分解为两部分：

趋势项：$X_T$=$EMA(x)$

季节性项：$X_s$=$X - X_T$



## 自适应融合的季节性MLP

为了在最小化噪声干扰的同时**有效捕捉弱季节性信号**，我们**将季节性信号划分为强、弱分量**，分别**通过频域MLP和标准MLP进行处理**。

**频域 MLP 与强季节性 MLP**: 采用了一种时频学习器（包含嵌入、实数 FFT、频域 MLP 和逆实数 FFT）来重构具有强季节性模式的特征 $f_{s1} \in \mathbb{R}^{C \times L \times E}$10。随后，一个强季节性 MLP 从 $f_{s1}$ 中学习预测值 $y_{21} \in \mathbb{R}^{Q \times C}$11

**弱季节性MLP**：时频学习器使用软阈值（softshrink）处理来降低噪声，但这不可避免地**丢失了低幅度的弱季节性信号**。因此，我们构建了一个**弱季节性 MLP**，直接从季节性信号  中提取弱季节性预测。



**AZCF（自适应零初始化通道融合）机制：** 现有的时间序列信号融合方法通常使用注意力机制在通道和时间维度上对信号进行加权14。给定强季节性预测 $y_{21}$ 和弱季节性预测 $y_{22}$，完整的季节性预测 $y_2$ 通常计算为 $y_2 = \alpha_1 \odot y_{21} + \alpha_2 \odot y_{22}$，其中权重是通过 Query-Key-Value 变换间接获得的。然而，这种间接方法引入了不必要的复杂性，且忽略了强弱季节性预测之间显著的信噪比差异。因此，我们提出了具有三个关键特征的 AZCF 机制：

**单参数融合 (Single-parameter fusion)**： 我们引入一个自适应权重系数 $\alpha$ 来调节弱季节性预测的贡献：
$$
y_2 = y_{21} + \alpha \odot y_{22}
$$
其中 $\alpha \in \mathbb{R}^{Q \times C}$。这种单参数融合首先确保了强信号的质量，然后选择性地增强弱信号，同时减少模型参数。

**通道维度融合 (Channel-dimension fusion)：** 由于 $y_{21}$ 和 $y_{22}$ 均通过通道独立方法获得，融合策略应尊重这一分离特性，在通道维度上进行独立决策，学习哪些特征拥有更可靠的弱季节性信号。沿时间维度融合会破坏季节性模式的时间连贯性，并可能在通道间传播噪声。因此，我们将 $\alpha$ 定义为 $\alpha = \{\alpha_1, ..., \alpha_C\} \in \mathbb{R}^{1 \times C}$，并通过广播机制将其扩展到输出时间步维度。

**零初始化 (Zero initialization)：** 我们将 $\alpha$ 初始化为零向量，从最可靠的强季节性预测开始：$\alpha_{init} = 0 \in \mathbb{R}^{1 \times C}$22。这给出了初始预测 $y_2^{init} = y_{21}$。



## Energy Invariant Attention（EIA）能量不变注意力

EIA (Energy Invariant Attention) 是一种**带物理约束的加权融合策略**。

**传统的基于分解的方法**通常通过直接将趋势预测 $y_1$ 和季节性预测 $y_2$ 相加来生成最终预测 $y_3$ ($y_3 = y_1 + y_2$) 。然而，这种方法可能限制了模型处理跨通道和时间步的、变化的、时间动态特征重要性的自适应能力。

**基于归一化注意力加权**的方法让模型学习一个参数$\beta$, $y_{final} = \beta \cdot y_{trend} + (1-\beta) \cdot y_{season}$, 预测出的信号幅度会**坍塌（Shrink）**，变得比真实值小很多。为了弥补这个幅度亏损，模型可能会被迫把 $\beta$ 学习得非常极端。

EIA能量不变注意力：
$$
y_{final} = \mathbf{2} \times [\beta \cdot y_{trend} + (1-\beta) \cdot y_{season}]
$$
**凸组合（加权平均）**在数学上隐含了一个“除以2”的归一化效果, EIA 通过乘以 2，把幅度**补偿**回来。



# ChatTS: Time Series LLM for Understanding and Reasoning

![image-20260109173906222](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260109173906222.png)

**1、保留值的时间序列归一化**

时间序列的数值特征至关重要，因为现实世界的应用通常涉及特定的数值查询（例如，询问最大 CPU 利用率），时间序列数据归一化可能导致丢失原始数据信息。

**方案**：首先，我们对**每个时间序列数组应用标准的最小 - 最大归一化**（0-1 缩放）。然后，**对于每个时间序列**，我们在文本中作为提示的一部分包含归一化参数 -“**值缩放**”（归一化期间的缩放因子）和 “**值偏移**”（归一化期间应用的偏移）。

**代码实现**：

① 归一化：计算均值并进行中心化，计算缩放因子

② 构建元数据提示词：它会根据**时间序列的统计信息**生成一段特殊的文本描述，`[offset=0.1234|scaling=1.0000|length=100|max=5.5|min=-2.1|left=0.1|right=0.5]<ts><ts/>`



`Processing_qwen3_ts.py`文件实现了：

① 实现了保留值的时间序列归一化

② 通过将`自然语言和原始时间序列`转化为`TokenID + 归一化时间序列张量`



**2、整体架构**

该模型采用了 **"Patch + Projection"** 的多模态融合范式（类似 Vision Transformer 或 LLaVA 处理图像的方式）：

1. **基座模型 (Backbone):** 使用 `Qwen3Model` 作为大脑，负责处理上下文和生成文本。
2. **编码器 (Encoder):** 使用一个轻量级的 `TimeSeriesEmbedding` (MLP结构) 将时间序列切片（Patch）并映射到 LLM 的特征空间（Embedding Space）。
3. **融合机制 (Fusion):** 在输入层（Input Embedding layer）直接将文本的 Embedding 和时间序列的 Embedding 拼接在一起。



# HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting

![image-20260112153909234](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260112153909234.png)

**现存方法的局限性**：

- *周期性建模隐含化*：传统趋势 - 季节性分解方法将周期性信息拆分到趋势和季节组件中，破坏了其跨时间尺度和空间位置的统一结构，无法直接建模连贯的周期性模式。
- *分解过程不可学习*：传统分解方法为固定规则驱动，效率低且易导致组件错位，难以适配复杂交通场景的动态变化。
- *单尺度周期性建模*：部分新兴方法（如 CycleNet）仅能处理单一尺度周期性，未充分利用多尺度周期信息，且缺乏对时空交互关系的有效建模。

## **HyperD 框架的创新**：

​	将交通数据**显式解耦**为周期性组件（**捕捉日、周等稳定周期模式**）和残差组件（**捕捉高频非规律波动**），通过专用模块分别建模，并引入损失函数保证组件语义分离，最终融合两部分输出得到预测结果。

## **混合周期性表示模块**

### 时空注意力编码器（STAE）: 捕获节点间空间关联和周期内长程时间依赖

**时间维度**：将每个时间步视为 Query，捕捉周期内部的长程演变规律 。

**空间维度**：将每个节点视为 Query，捕捉不同位置之间超越物理连接的全局空间模式 。

### 混合周期模式

**元数据准备**：对于每一个时间步（Time Step），模型需要知道两个信息:

- **Time of Day (ToD)**：现在是一天中的第几个时间片？（例如：14:30 是第几步）
- **Day of Week (DoW)**：今天是周几？（例如：周一、周二...）

**索引计算**: 

​	**每日索引 ($i_D$)**：直接等于 `ToD`。它负责在 $\tilde{P}_D$（每日嵌入）中定位。比如“早高峰”在每天的索引位置是固定的。

​	**每周索引 ($i_W$)**：计算公式为 $i_W = \text{ToD} + \text{DoW} \times L_D$。

​	每周嵌入 $\tilde{P}_W$ 实际上是一个长度为 $7 \times 288$ 的长向量（或矩阵）。这个公式将二维的“周几+时间”展平成了一维索引，从而区分“周一的早高峰”和“周日的早高峰”。

**检索与聚合**：

针对**$S^{in}$**：

- 使用 $i_D$ 从每日嵌入 $\tilde{P}_D$ 中切片，得到 $S_D$（每日分量）
- 使用 $i_W$ 从每周嵌入 $\tilde{P}_W$ 中切片，得到 $S_W$（每周分量）
- 直接相加

 针对$S^{out}$:

- **确定时间**：对于未来要预测的每一个时间点 $t$（从 $T_1+1$ 到 $T_1+T_2$）。
- **检索嵌入（Retrieve）**：
  - 根据未来的时间点（比如“明天上午8:00”），去**每日嵌入表** $\tilde{P}_D$ 中查出对应的特征向量 $S^{out}_D$ 。
  - 根据未来的日期（比如“明天是周五”），去**每周嵌入表** $\tilde{P}_W$ 中查出对应的特征向量 $S^{out}_W$ 。
- **聚合（Aggregate）**：将两者相加，得到 $S^{out} = S^{out}_D + S^{out}_W$ 。



## **频率感知残差表示模块**

### 时空频率编码器（STFE）

将残差转换至频域，通过复数值MLP建模频率特异性时空行为，再转回时域，高效捕捉高频动态。

流程概览： `输入残差 -> 升维 -> 空间 FFT -> 复数 MLP -> 空间 IFFT -> 时间 FFT -> 复数 MLP -> 时间 IFFT -> 输出`



## 双视角对齐损失

为避免周期性组件与残差组件语义重叠，保证解耦有效性：

**频率分割**：对最终预测结果 $\hat{Y}$（它是 $S^{out}$ 和 $R^{out}$ 的和）执行FFT，按阈值 $F_{low}$ 分割为低频部分($\hat{Y}_f^{low}$)（对应周期性模式）和高频部分($\hat{Y}_f^{high}$) （对应残差模式）。

**对齐约束**：通过**MSE损失**让低频部分与 $S^{out}$对齐，高频部分与$R^{out}$对齐，公式为 $$\text{Loss} = \text{MSE}(\text{低频分量}, S^{out}) + \text{MSE}(\text{高频分量}, R^{out})$$ 。

**总损失**：$\mathcal{L} = \mathcal{L}_{pred} + \alpha * \mathcal{L}_{dva}$，其中 $\alpha$ 是一个权重系数，用来平衡预测准确度和解耦程度。其中，**预测损失 ($\mathcal{L}_{pred}$)**：预测值 $\hat{Y}$ 和真实值 $Y^{gt}$ 之间的差异。



# Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting

![image-20260116174804252](C:\Users\Xiaoming\AppData\Roaming\Typora\typora-user-images\image-20260116174804252.png)

## 可学习的小波变化

将时间序列数据的嵌入表示（Embedding）转换到 **小波空间（Wavelet Space）**。这样做的好处是能够同时保留 **时间信息** 和 **频率信息**，从而让模型既能捕捉到局部的细节变化（高频），也能捕捉到整体的长期趋势（低频）。
$$
M_k = \exp(-w_{\alpha}t^2) \times \cos(w_{\beta}t + w_{\gamma}t^2)
$$
定义了一组 $K$ 个可学习的小波变换基函数，称为“原子”**（Atoms）**，记为矩阵 $M_k$。通过三个可学习的权重向量来控制波形的形状：

**高斯包络（Gaussian Envelope）：** $\exp(-w_{\alpha}t^2)$， $w_{\alpha}$ 控制波形的宽度（即“包络”）。它让波形在时间轴上局域化，只在特定时间段内有较大的值，而不是无限延伸。

**余弦波形（Cosine Waveform）：** $\cos(w_{\beta}t + w_{\gamma}t^2)$，线性频率调节 ($w_{\beta}$)： 决定了基础震荡频率，二次频率调节 ($w_{\gamma}$)：允许频率随时间发生变化。

**传统的小波变换**通常**使用固定形状的小波**（如 Morlet 小波），而在这里，$w_{\alpha}, w_{\beta}, w_{\gamma}$ 都是随机初始化的参数，模型会在训练过程中自动调整它们，以生成最适合当前数据特征的小波形状。

**多分辨率分解**：利用$K$个原子学习获得了**不同宽窄（时间分辨率）**和**不同快慢（频率分辨率）**的形态。



## 多变量相干注意力机制（Multivariable Coherence Attention, MVCA）

传统的 Transformer 注意力机制**通过“点积”来计算相似度**，而 MVCA 则是通过**“谱相干性”（Spectral Coherence）**来计算变量之间的依赖关系。它的核心假设是：**如果在频域上两个变量的谱相干性越高，它们之间的关联就越强，应该被分配更多的注意力权重** 。

1、**特征映射**：将输入 $P$ 投影生成 Query ($Q$)、Key ($K$) 和 Value ($V$) 。

​	**多头分配**：利用小波变换的 $K$ 个原子。**每一个小波原子对应一个注意力头（Head）**。每个头处理不同的时间-频率分辨率下的特征子空间。

2、**频域转换**：对每个头的 $Q$ 和 $K$ 进行快速傅里叶变换（FFT），变换后的每个频率区间都包含了所有原始输入变量的信息，从而能捕捉从**细粒度（高频）**到**全局（低频）**的模式。

3、**计算谱相干性**：用于衡量 $Q$ 和 $K$ 在频域上的线性依赖关系。

- **计算互谱和功率谱：**
  - **互谱密度 ($P_{qk}$)**：$Q$ 与 $K$ 的共轭复数相乘，表示两者的交互信息。
  - **功率谱密度 ($P_{qq}, P_{kk}$)**：$Q$ 和 $K$ 各自与自身的共轭相乘，表示各自的能量。
- **计算相干性分数 ($C_{qk}$)：**

​	公式为：
$$
C_{qk} = \frac{|P_{qk}|^2}{P_{qq} \cdot P_{kk} + \epsilon}
$$
​	这本质上是一个**频域上的归一化相关系数**。

​	如果 $C_{qk}$ 的值大，说明在当前时间步，Query 和 Key 在多个频率波段上的变化趋势高度一致（相干）。

## Koopman 引导的频谱演化

**Koopman 算子**理论的核心观点是：一个在原始状态空间中高度非线性的动力系统，可以通过映射到一个无限维的观测空间（这里是小波变换后的频域空间），变成一个**线性的动力系统** 。**目的是**捕捉小波空间中时频模式的**时间演化**。模型试图学习一个算子 $\mathcal{K}$，通过简单的线性矩阵乘法来模拟复杂的时间推演。

**算子 $\mathcal{K}$ 的构造过程**

1、**初始化与正交化（保证能量守恒）：**

- 首先初始化一个可学习的复数矩阵 $S$。
- 对其进行 **QR 分解**，只保留 **酉矩阵（Unitary Matrix）** $U$。
- **原因：** 酉矩阵满足 $U^{\dagger}U = I$。在数学上，乘以酉矩阵只进行旋转而不改变向量的模长（能量）。这能有效防止在深层网络中常见的 **梯度爆炸（数据放大）或梯度消失（数据扭曲）** 问题。

2、**特征值构造（控制演化速度）：**

- 初始化一个可学习的实数向量 $p$，其中的元素 $p_k$ 控制第 $k$ 个变换的 **相位演化角度** 。
- 将其映射到复数单位圆上：$v_k = e^{ip_k}$ 。
- 构造对角矩阵 $D = \text{diag}(v)$ 。
- **物理意义：** 在频域中，乘以 $e^{ip_k}$ 相当于改变相位，这在时域上对应着 **时间的平移**。这就是模型“预测未来”的数学本质。

3、**生成算子：**

- Koopman 算子被定义为谱分解形式：$\mathcal{K} = U D U^{\dagger}$ 。

Koopman算子的预测过程指的是通过**一次性的全局投影避免了序列化预测带来的误差累积**。
